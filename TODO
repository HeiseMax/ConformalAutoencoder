Code Improvements:
- save optimizer and scheduler in model
- README is autogenerated and not checked.
- validation should calculate conformality metric (also det estimate?)
- can the gradient of the first reconstruction pass be used for the conformal loss?
- What call in code/training takes how much time?
- make reg_in_loss a parameter of the train function


Metrics:
- review and update all metric calculations
    if loss and reg need to be calculated, calculate jacobian only once and reuse.
    General Hyperparameters, losses, reg, etc as parameters for AE: num_samples for trace estimate (conform/regularization)
    Change metric functions to take in quantities like jacobian etc. and make everything more modular
    SHould losses be calculated with mean or norm or straight, squared?
- test trace loss with unnormalized vectors
    For every loss, choice of vectors (angle, length) should be investigated
- Use Riemannian metric (J(z)^TJ(z)) to get geodesics -> interpolation.
- trace loss requires mean over batch in fraction, would it be useful to have batch of batches to get the mean loss over multiple batches?


Further research:
- Try a transformer architecture
- Try residual connections
- Try a conditional model (maybe MNIST again?)
- Is there a way to improve the estimate of c by using off diagonal elements? (https://chatgpt.com/share/6894cae7-29a4-800a-81d5-c18454d53609)
- Matrix Perturbation: https://arxiv.org/abs/2002.05001 (λi​=c+δMii​+j=i∑​c−λj​∣δMij​∣2​+⋯)
- Woodbury? and others (chatgpt discussion)
- Whitney embedding theorem (in what way is this useful, tie in into the theory)
- Influence of num samples in trace estimation
- Metrics for "niceness" of latent space
- compare conformal factor std between iso and conf
- What c-values does scaled iso take?
- Manifold overfitting can be fixed by noise, should I also train with noise?
- ablation of lambda_aug
- nnU-net: how do they automatically  configure their networks?
