Code Improvements:
- save optimizer and scheduler in model
- Move augmentation to Autoencoder? Make wrapper function
- General Hyperparameters, losses, reg, etc as parameters for AE: num_samples for trace estimate (conform/regularization)
- README is autogenerated and not checked.


Metrics:
- review and update all metric calculations
- if loss and reg need to be calculated, calculate jacobian only once and reuse.
- test trace loss with unnormalized vectors
- Use Riemannian metric (J(z)^TJ(z)) to get geodesics -> interpolation.
- memory explodes for celebA jacobian calculation (50GB per sample)


Further research:
- Is there a way to improve the estimate of c by using off diagonal elements? (https://chatgpt.com/share/6894cae7-29a4-800a-81d5-c18454d53609)
- Matrix Perturbation: https://arxiv.org/abs/2002.05001 (λi​=c+δMii​+j=i∑​c−λj​∣δMij​∣2​+⋯)
- Woodbury? and others (chatgpt discussion)
- Try a conditional model (maybe MNIST again?)
- Whitney embedding theorem (in what way is this useful, tie in into the theory)
- Influence of num samples in trace estimation
- Metrics for "niceness" of latent space



- Training needs to be more efficient. Calculate trace once and use for loss and reg.
- Validation should check conformality metric with 10-20 samples. (prob 1,5 min per sample on celebA)